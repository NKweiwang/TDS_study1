---
title: "Home"
output:
  html_document:
    toc: false
---

## 1. Aim

The aim of this case study is to predict the binary response from binary data. We want to provide a useful tool to do comparison among different kinds of methods with different scores. 

There is no perfect model, but there should be a better model. The goal of this project is to find a better model to specific data, and make this process easily to use and extend when we find other powerful methods.

## 2. Models

We can consider this problem as a classification problem. So the following things in our data analysis is how to fit the model and how to assess the performance of the model we choose. There is no perfect model for all the data, but we can choose one proper model for this specific data out of a couple of methods we consider.

### 2.1 Classification

As classification problem, we consider some popular methods:

#### 2.1.1 Random Forest

Using the Rpackage `randomForest`

#### 2.1.2 Support Vector Machaine

Using the Rpackage `e1071`

#### 2.1.3 logistic Regression

Using the Rpackage `glmnet`

### 2.2 Classification with the feature learned

Take the logistic regression as example. 

\begin{eqnarray}
Y_i &\sim& Bernoulli(logit(\mu_i))\\
\mu_i &=& X_i \beta
\end{eqnarray}

There are too many variables (16K+) than the sample size (~500). The values of the variables are highly compressed ($Y_i = \pm1$), which can lead to colinearality among variables. We can deal with this problem with variable selection and dimension reduction techniques.

Variable selection can be done in `glmnet` package, which is straightforward. Random Forest can also deal with the problem colinearality since it subsample the variables each time.

For Dimension reduction, we can use the PC from PCA or factor (feature) from topic model, which is for count data. 

This idea is from principal component regression (PCR) which deals with the colinearality in linear regression. 

In PCR, instead of running regression model with $X$, we use $Xv_K$ as the predictor (new $X$ variables). The $v_K$ is calculate from the PCA: $X = U \Lambda V^T$ and $v_K$ is the first K columns of $V$.

The advantage is not only fix shrinkage problem of variance of the estimation, but also accelerate the computation. Under our framework, we can only use K variables to do SVM or Random Forest, which can much more computationally efficient.

From the view of dimension reduction, we reduce the dimension from 1600+ to K, which usually I choose 10-20 in this data. The value of K is based on the eigen values from PCA.

For Dimension reduction, there are several ways to do it:

PCA: use first few PCs as $X$. `svd`

PMA: use first few features as $X$. `PMA`

Topic Model(LDA): use the features learned from topic model as $X$. `maptpx` (the reason I choose Topic model to learn the feature is out data is count data).

There is a factor model just for binary data.[Polya-Gamma augmentations for factor models](http://www.jmlr.org/proceedings/papers/v39/klami14.pdf) which can be consider in the future.


## 3 Modules of the Functional Parts

This part is to introduce the code part. We modularize different parts of work to make reproducible report and reusable code. And we also make the structure of the code easy to maintain and extend.

### 3.1 Classification Methods

All the classification methods are in `/code/Cfunctions.R`.

All the methods have same format of input and output. Users just need add some more methods they would like in the same format. And then, those methods would be called in function `fold_compare`. Every users want to switch from different methods, they just need to set on parameter `method=` in `fold_compare`.

### 3.2 Feature Learning Methods

All the feature learning methods are in `/code/Ffunctions.R`

All the methods have same format of input and output. Users just need add some more methods they would like in the same format. There are many factor model, so this make it easy to try other factor model under this problem. 

### 3.3 Model Assesment

All the score functions are in `/code/Sfunctions.R`

All the score functions are in the same format with input containing `Y` as $Y_test$ and `f` as $\hat{p}$. This make it easy to add more score in the future.

#### 3.3.1 Score Function

There are several score function we consider about:

##### hinge loss

\begin{eqnarray}
max(0,(2Y_i - 1)(2 sign(p_i > 1/2) -1))
\end{eqnarray}

##### square loss

\begin{eqnarray}
Y_i (1-p_i)^2 + (1-Y_i) p_i^2
\end{eqnarray}

##### cross entropy loss

\begin{eqnarray}
- Y_i \log(p_i) - (1-Y_i)\log(1 - p_i)
\end{eqnarray}

#### 3.3.2 Cross-Validation

The idea of Cross-Validation (CV) is to predict part of the data with others.
\begin{eqnarray}
MSE = \frac{1}{N}\sum_i L(Y_i, p_i(X_{-i})) 
\end{eqnarray}

For computational efficiency, we can also use 5-fold CV, which leave 20 percent out each time rather than leave one out.

We use CV with different score function $L(Y_i, p_i(X_{-i}))$ to assess the performance.

## 4. Data Anlysis 
There are 3 classification methods and 3 feature learning methods and 3 score functions we propose. So there are $3 \times (3+1) \times 3 = 36$ possible assessments on $3 \times (3+1) = 12$ methods. 

Due to the time limit, I can not run a cross validation on each of these combinations. I just run CV on some cases. All other method are compared with each other based on two data sets (training set and test set). The training set are randomly chosen from the original data, and rest of the data goes to test set. One can easily extend this comparison following the structure of this project and do the CV to get better idea which method works better in which situation.

### 4.1 Classification

Click on this [Logistic Regression](logistic.html) to see my results.

Click on this [Support Vector Machaine](SVM.html) to see my results.

Click on this [Random Forest](RandomF.html) to see my results.

The current result shows that SVM has better performance on hinge loss than others, and logistic regression has better performance on cross entropy loss. The results make sense since SVM uses hinge loss as objective function and logistic regression's likelihood is the negative value of cross entropy loss. Lack of observations in random forest (the CV of random forest run too long), I can not draw any conclusion on it.

### 4.2 Classification with Feature Learning

Click on this [Classification based on Features](logistic_F.html) to see my results.

The results show that Topic Model might provide better features than others, although CV is also needed to be done to check the performance across all the methods. I tried $K = 10$ which not shown and $K=20$. I would guess that we might want to try larger rank (more features). 

## 5. Conclusion

- Different methods work better based on the choice of score function. If users are interested in cross entropy loss, we would suggest logistic regression. If users are interested in hinge loss, SVM could be better choice.

- Dimension reduction can be fast due to much less dimensionality, but the performance is not as good as the method using original data. We can try larger rank to see how good can this approximation be.

## 6. Future Work

- To run CV on more situations and use table or boxplot to show the results.

- Try more rank for Topic Model in different methods.

## 7. Summary

### 7.1 Method selection

There is no right answer in method selection. It is depends on different data set, loss function. Our tool is provide a way to check the performance of methods in different cases and make suggestion for specific situation.

### 7.2 Exploration in Feature Learning

This structure is easy to extend. One example is the exploration we did in feature learning. The motivation is from principal component regression and dimension reduction to reduce the computational complexity. We found that the performance is not as good as the method using the original data. Topic model seem work better in this data. We might need to try more rank (more features). 

No matter if the classification with feature learning is a good idea or not on this data, this study show the capability of our tool to extend new methods and try new ideas out. 

### 7.3 Modularizing Problem

We put all the classification methods in to `/code/Cfunctions` with same format (input and output). This unified framework are easy to extend by adding more methods and also easy to be applied other procedures.

We build a unified frame work to run different methods with different scores to assess the performance. For simple comparison, user just need use `fold_compare` and set up the data, method type and the index for training or test data set. For cross validation experiment, user just need apply `CV_compare` by setting data and method type.

It is easy to add new method and score in to the comparison. If we get another data set or another method in the future, we can easily add them to comparison to decide which method is best under certain score for specific data set.
