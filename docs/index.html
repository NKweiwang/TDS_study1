<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Home</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">DS study1</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://nkweiwang.github.io/TDS_study1">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Home</h1>

</div>


<div id="aim" class="section level2">
<h2>1. Aim</h2>
<p>The aim of this case study is to predict the binary response from binary data. We want to provide a useful tool to do comparison among different kinds of methods with different scores.</p>
<p>There is no perfect model, but there should be a better model. The goal of this project is to find a better model to specific data, and make this process easily to use and extend when we find other powerful methods.</p>
</div>
<div id="models" class="section level2">
<h2>2. Models</h2>
<p>We can consider this problem as a classification problem. So the following things in our data analysis is how to fit the model and how to assess the performance of the model we choose. There is no perfect model for all the data, but we can choose one proper model for this specific data out of a couple of methods we consider.</p>
<div id="classification" class="section level3">
<h3>2.1 Classification</h3>
<p>As classification problem, we consider some popular methods:</p>
<div id="random-forest" class="section level4">
<h4>2.1.1 Random Forest</h4>
<p>Using the Rpackage <code>randomForest</code></p>
</div>
<div id="support-vector-machaine" class="section level4">
<h4>2.1.2 Support Vector Machaine</h4>
<p>Using the Rpackage <code>e1071</code></p>
</div>
<div id="logistic-regression" class="section level4">
<h4>2.1.3 logistic Regression</h4>
<p>Using the Rpackage <code>glmnet</code></p>
</div>
</div>
<div id="classification-with-the-feature-learned" class="section level3">
<h3>2.2 Classification with the feature learned</h3>
<p>Take the logistic regression as example.</p>
<span class="math display">\[\begin{eqnarray}
Y_i &amp;\sim&amp; Bernoulli(logit(\mu_i))\\
\mu_i &amp;=&amp; X_i \beta
\end{eqnarray}\]</span>
<p>There are too many variables (16K+) than the sample size (~500). The values of the variables are highly compressed (<span class="math inline">\(Y_i = \pm1\)</span>), which can lead to colinearality among variables. We can deal with this problem with variable selection and dimension reduction techniques.</p>
<p>Variable selection can be done in <code>glmnet</code> package, which is straightforward. Random Forest can also deal with the problem colinearality since it subsample the variables each time.</p>
<p>For Dimension reduction, we can use the PC from PCA or factor (feature) from topic model, which is for count data.</p>
<p>This idea is from principal component regression (PCR) which deals with the colinearality in linear regression.</p>
<p>In PCR, instead of running regression model with <span class="math inline">\(X\)</span>, we use <span class="math inline">\(Xv_K\)</span> as the predictor (new <span class="math inline">\(X\)</span> variables). The <span class="math inline">\(v_K\)</span> is calculate from the PCA: <span class="math inline">\(X = U \Lambda V^T\)</span> and <span class="math inline">\(v_K\)</span> is the first K columns of <span class="math inline">\(V\)</span>.</p>
<p>The advantage is not only fix shrinkage problem of variance of the estimation, but also accelerate the computation. Under our framework, we can only use K variables to do SVM or Random Forest, which can much more computationally efficient.</p>
<p>From the view of dimension reduction, we reduce the dimension from 1600+ to K, which usually I choose 10-20 in this data. The value of K is based on the eigen values from PCA.</p>
<p>For Dimension reduction, there are several ways to do it:</p>
<p>PCA: use first few PCs as <span class="math inline">\(X\)</span>. <code>svd</code></p>
<p>PMA: use first few features as <span class="math inline">\(X\)</span>. <code>PMA</code></p>
<p>Topic Model(LDA): use the features learned from topic model as <span class="math inline">\(X\)</span>. <code>maptpx</code> (the reason I choose Topic model to learn the feature is out data is count data).</p>
<p>There is a factor model just for binary data.<a href="http://www.jmlr.org/proceedings/papers/v39/klami14.pdf">Polya-Gamma augmentations for factor models</a> which can be consider in the future.</p>
</div>
</div>
<div id="modules-of-the-functional-parts" class="section level2">
<h2>3 Modules of the Functional Parts</h2>
<p>This part is to introduce the code part. We modularize different parts of work to make reproducible report and reusable code. And we also make the structure of the code easy to maintain and extend.</p>
<div id="classification-methods" class="section level3">
<h3>3.1 Classification Methods</h3>
<p>All the classification methods are in <code>/code/Cfunctions.R</code>.</p>
<p>All the methods have same format of input and output. Users just need add some more methods they would like in the same format. And then, those methods would be called in function <code>fold_compare</code>. Every users want to switch from different methods, they just need to set on parameter <code>method=</code> in <code>fold_compare</code>.</p>
</div>
<div id="feature-learning-methods" class="section level3">
<h3>3.2 Feature Learning Methods</h3>
<p>All the feature learning methods are in <code>/code/Ffunctions.R</code></p>
<p>All the methods have same format of input and output. Users just need add some more methods they would like in the same format. There are many factor model, so this make it easy to try other factor model under this problem.</p>
</div>
<div id="model-assesment" class="section level3">
<h3>3.3 Model Assesment</h3>
<p>All the score functions are in <code>/code/Sfunctions.R</code></p>
<p>All the score functions are in the same format with input containing <code>Y</code> as <span class="math inline">\(Y_test\)</span> and <code>f</code> as <span class="math inline">\(\hat{p}\)</span>. This make it easy to add more score in the future.</p>
<div id="score-function" class="section level4">
<h4>3.3.1 Score Function</h4>
<p>There are several score function we consider about:</p>
<div id="hinge-loss" class="section level5">
<h5>hinge loss</h5>
<span class="math display">\[\begin{eqnarray}
max(0,(2Y_i - 1)(2 sign(p_i &gt; 1/2) -1))
\end{eqnarray}\]</span>
</div>
<div id="square-loss" class="section level5">
<h5>square loss</h5>
<span class="math display">\[\begin{eqnarray}
Y_i (1-p_i)^2 + (1-Y_i) p_i^2
\end{eqnarray}\]</span>
</div>
<div id="cross-entropy-loss" class="section level5">
<h5>cross entropy loss</h5>
<span class="math display">\[\begin{eqnarray}
- Y_i \log(p_i) - (1-Y_i)\log(1 - p_i)
\end{eqnarray}\]</span>
</div>
</div>
<div id="cross-validation" class="section level4">
<h4>3.3.2 Cross-Validation</h4>
The idea of Cross-Validation (CV) is to predict part of the data with others.
<span class="math display">\[\begin{eqnarray}
MSE = \frac{1}{N}\sum_i L(Y_i, p_i(X_{-i})) 
\end{eqnarray}\]</span>
<p>For computational efficiency, we can also use 5-fold CV, which leave 20 percent out each time rather than leave one out.</p>
<p>We use CV with different score function <span class="math inline">\(L(Y_i, p_i(X_{-i}))\)</span> to assess the performance.</p>
</div>
</div>
</div>
<div id="data-anlysis" class="section level2">
<h2>4. Data Anlysis</h2>
<p>There are 3 classification methods and 3 feature learning methods and 3 score functions we propose. So there are <span class="math inline">\(3 \times (3+1) \times 3 = 36\)</span> possible assessments on <span class="math inline">\(3 \times (3+1) = 12\)</span> methods.</p>
<p>Due to the time limit, I can not run a cross validation on each of these combinations. I just run CV on some cases. All other method are compared with each other based on two data sets (training set and test set). The training set are randomly chosen from the original data, and rest of the data goes to test set. One can easily extend this comparison following the structure of this project and do the CV to get better idea which method works better in which situation.</p>
<div id="classification-1" class="section level3">
<h3>4.1 Classification</h3>
<p>Click on this <a href="logistic.html">Logistic Regression</a> to see my results.</p>
<p>Click on this <a href="SVM.html">Support Vector Machaine</a> to see my results.</p>
<p>Click on this <a href="RandomF.html">Random Forest</a> to see my results.</p>
<p>The current result shows that SVM has better performance on hinge loss than others, and logistic regression has better performance on cross entropy loss. The results make sense since SVM uses hinge loss as objective function and logistic regression’s likelihood is the negative value of cross entropy loss. Lack of observations in random forest (the CV of random forest run too long), I can not draw any conclusion on it.</p>
</div>
<div id="classification-with-feature-learning" class="section level3">
<h3>4.2 Classification with Feature Learning</h3>
<p>Click on this <a href="logistic_F.html">Classification based on Features</a> to see my results.</p>
<p>The results show that Topic Model might provide better features than others, although CV is also needed to be done to check the performance across all the methods. I tried <span class="math inline">\(K = 10\)</span> which not shown and <span class="math inline">\(K=20\)</span>. I would guess that we might want to try larger rank (more features).</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>5. Conclusion</h2>
<ul>
<li><p>Different methods work better based on the choice of score function. If users are interested in cross entropy loss, we would suggest logistic regression. If users are interested in hinge loss, SVM could be better choice.</p></li>
<li><p>Dimension reduction can be fast due to much less dimensionality, but the performance is not as good as the method using original data. We can try larger rank to see how good can this approximation be.</p></li>
</ul>
</div>
<div id="future-work" class="section level2">
<h2>6. Future Work</h2>
<ul>
<li><p>To run CV on more situations and use table or boxplot to show the results.</p></li>
<li><p>Try more rank for Topic Model in different methods.</p></li>
</ul>
</div>
<div id="summary" class="section level2">
<h2>7. Summary</h2>
<div id="method-selection" class="section level3">
<h3>7.1 Method selection</h3>
<p>There is no right answer in method selection. It is depends on different data set, loss function. Our tool is provide a way to check the performance of methods in different cases and make suggestion for specific situation.</p>
</div>
<div id="exploration-in-feature-learning" class="section level3">
<h3>7.2 Exploration in Feature Learning</h3>
<p>This structure is easy to extend. One example is the exploration we did in feature learning. The motivation is from principal component regression and dimension reduction to reduce the computational complexity. We found that the performance is not as good as the method using the original data. Topic model seem work better in this data. We might need to try more rank (more features).</p>
<p>No matter if the classification with feature learning is a good idea or not on this data, this study show the capability of our tool to extend new methods and try new ideas out.</p>
</div>
<div id="modularizing-problem" class="section level3">
<h3>7.3 Modularizing Problem</h3>
<p>We put all the classification methods in to <code>/code/Cfunctions</code> with same format (input and output). This unified framework are easy to extend by adding more methods and also easy to be applied other procedures.</p>
<p>We build a unified frame work to run different methods with different scores to assess the performance. For simple comparison, user just need use <code>fold_compare</code> and set up the data, method type and the index for training or test data set. For cross validation experiment, user just need apply <code>CV_compare</code> by setting data and method type.</p>
<p>It is easy to add new method and score in to the comparison. If we get another data set or another method in the future, we can easily add them to comparison to decide which method is best under certain score for specific data set.</p>
</div>
</div>

<hr>
<p>
    This <a href="http://rmarkdown.rstudio.com">R Markdown</a> site was created with <a href="https://github.com/jdblischak/workflowr">workflowr</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
